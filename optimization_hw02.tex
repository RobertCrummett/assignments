\footline={2/21/2025\hfill Page \folio}
\def\reals{I\kern-4pt R}
\def\nats{I\kern-4pt N}
\let\oldexists\exists \def\exists{\oldexists \;}
\let\oldforall\forall \def\forall{\oldforall \,}
\def\qed{\vrule height 6pt width 6pt depth 0pt}
\parindent 0pt
\parskip 2mm


EENG~521: Homework~2\hfill Robert~Nate~Crummett
\smallskip
\hrule


\medskip{\bf Problem 1}


{\bf a)}\hskip2mm
Let $f(x)=100(x_2-x_1^2)^2+(1-x_1)^2$.
The gradient $\nabla f(x)\in\reals^2$ and the Hessian $\nabla^2f(x)\in\reals^{2\times 2}$ are
$$\eqalignno{\nabla f(x)&=\pmatrix{-400x_1(x_2-x_1^2)-2(1-x_1)\cr200(x_2-x_1^2)}&\cr\nabla^2f(x)&=\pmatrix{-400x_2+1200x_1^2+2&-400x_1\cr-400x_1&200}&\qed\hskip3pt{}}$$


To prove $x^*=(1\;1)$ is the only stationary point, it is sufficient to show that it is the unique solution to the linear system $\nabla f(x)=0$.
If a solution exists, it is unique, because there are two equations in two unknowns
$$\eqalignno{400x_1^3-400x_1x_2+2x_1-2&=0&(1)\cr-200x_1^2+200x_2&=0&(2)}$$
Equation (2) implies that $x_2=x_1^2$.
Substitution of this equality into (1) yields $x_1=1$.
Plugging this value back into (2), I find that $x_2=1$.
This yields the desired result.\hfill\qed\hskip3pt{}


To show $x^*$ is the unique local minimizer, the Hessian must be positive definite.
$$\nabla^2f(x^*)=\pmatrix{\hphantom{-}802&-400\cr-400&\hphantom{-}200}\eqno(3)$$
The eigenvalues of this matrix are $\lambda_1\approx1001.6$ and $\lambda_2\approx0.3994$.
Positive eigenvalues mean $\nabla^2f(x)\succ0$\hfill\qed\hskip3pt{}


{\bf P.S.\ }This matrix is quite poorly conditioned.
This is evident both from the large distance between the eigenvalues and the similarity in the rows of (3).


{\bf b)}\hskip2mm
Let $f(x)=8x_1+12x_2+x_1^2-2x_2^2$.
Show that there is one stationary point, and that it is a saddle point.


As before, let $\nabla f(x)=0$.
$$\nabla f(x)=\pmatrix{8+2x_1\cr12-4x_2}=0$$
Clearly, there is only one stationary point: $x^*=(-4\;3)$.
This is the first requested result.\hfill\qed\hskip3pt{}


To show $x^*$ is a saddle point, the eigenvalues of the Hessian $\nabla^2f(x^*)$ must be examined.
$$\nabla^2f(x)=\pmatrix{2&\hphantom{-}0\cr0&-4}\eqno(4)$$
The Hessian (4) has no dependence on $x$.
Therefore $\nabla^2f(x)=\nabla^2f(x^*)$.
The Hessian (4) is also already diagonal.
By inspection, the eigenvalues are $\lambda_1=2$ and $\lambda_2=-4$.
Because the eigenvalues have opposite signs, the stationary point $x^*$ must lie on a saddle.
This is the second requested result.\hfill\qed\hskip3pt{}


{\bf c)}\hskip2mm
Let $f(x)=e^{x_1+x_2}+(x_1-x_2)^2$. The gradient $\nabla f(x)$ is
$$\nabla f(x)=\pmatrix{e^{x_1+x_2}+2(x_1-x_2)\cr e^{x_1+x_2}-2(x_1-x_2)}$$
Stationary points, where $\nabla f(x)=0$, never occur.
To see this, let each component of the gradient be equal to zero.
$$\eqalign{e^{x_1+x_2}+2(x_1-x_2)=0\cr e^{x_1+x_2}-2(x_1-x_2)=0}$$
If the equations are added together, the polynomial terms cancel and the exponential terms only remain.
$$(e^{x_1+x_2}+2(x_1-x_2))+(e^{x_1+x_2}-2(x_1-x_2))=2e^{x_1+x_2}=0$$
But the exponential is positive for all points $x_1$ and $x_2$.
So the last equality has no solutions.
Therefore there are no stationary points of this function.
Therefore there is no global minimizer.\hfill\qed\kern3pt

\eject
{\bf Problem 2}


{\bf a)}\hskip2mm
To show that $f$ is convex, it's sufficient to show the Hessian is positive semi-definite.
The Hessian of $f$ is
$$\nabla^2f(x)=A^TA$$
For general $A$, the Hessian $A^TA$ will be positive semi-definite. Therefore $f$ is convex, which is the requested result.\hfill\qed\kern3pt

{\bf b)}\hskip2mm
The gradient of $f$ is
$$\nabla f(x)=A^TAx-A^Ty.$$
By definition, the global minimizer of a convex function is at a singular point.
If $\nabla f(x)=0$, $A^TAx-A^Ty=0$ which leads to the linear system given.
Irrespective if the solution is unique or not, any $x$ satisfying this linear system is a minimizer of the function $f$.
Since $f$ is convex, any minimizer is equal to the global minimizer.\hfill\qed\kern3pt

{\bf c)}\hskip2mm
If $A^TA$ is singular (which possible since the Hessian of $f$ can only be guaranteed to be a positive semi-definite matrix), the linear system has no unique solution.
But if $A^TA$ has full column rank, it is non-singular.
Therefore the Hessian will be invertible and the linear system has a unique solution.
In this case, the conclusion above can be strengthened: for a Hessian with full column-rank, a solution $x$ to the linear system is the unique global minimizer.\hfill\qed\kern3pt


\medskip{\bf Problem 3}


{\bf a)}\hskip2mm
The function $f(u,v)={1\over2}\|A-uv^T\|^2_F$ can be rewritten as
$$f(u,v)={1\over2}{\rm tr}((A-uv^T)^T(A-uv^T))={1\over2}{\rm tr}(A^TA-vu^TA-A^Tuv^T+vu^Tuv^T)$$
Now the gradient of $f(u,v)$ with respect to the vector $u$ is
$$\eqalign{\nabla_uf(u,v)&={1\over2}\nabla_u({\rm tr}(A^TA)-{\rm tr}(vu^TA)-{\rm tr}(A^Tuv^T)+{\rm tr}(vu^Tuv^T))\cr&=-{1\over2}\nabla_u{\rm tr}(vu^TA)-{1\over2}\nabla_u{\rm tr}(A^Tuv^T)+{1\over2}\nabla_u{\rm tr}(vu^Tuv^T))\cr&=-{1\over2}\nabla_uu^TAv-{1\over2}\nabla_uv^TA^Tu+{1\over2}\nabla_uu^Tuv^Tv\cr&=-{1\over2}Av-{1\over2}Av+v^Tvu\cr&=-Av+{1\over2}(v^Tv)u}$$
The second line follows from breaking apart the trace.
The third line follows from the cyclic property of the trace.
Each term has been rotated such that it is equal to a scalar quantity.
Each argument being a scalar quantity, the trace can be discarded.
After applying the gradient with respect to $u$, a final expression for $\nabla_uf(x)$ is arrived at.
In very much the same manner, an expression for the gradient of $f$ with respect to $v$ is derived:
$$\nabla_vf(u,v)=-u^TA+{1\over2}(u^Tu)v$$
Combining the two expressions yeilds the gradient of $f$
$$\nabla f(u,v)=\left[\matrix{\nabla_uf(u,v)\cr\noalign{\kern2pt}\nabla_vf(u,v)}\right]=\left[\matrix{-Av+{1\over2}(v^Tv)u\cr\noalign{\kern2pt}-u^TA+{1\over2}(u^Tu)v}\right]\eqno\qed\kern3pt$$

\eject
{\bf b)}\hskip2mm
To locate the global minimizers, it is first necessary to find the stationary points.
The stationary points of $f(u,v)$ will be where the gradient is equal to zero
$$\nabla f(u,v)=\left[\matrix{\nabla_uf(u,v)\cr\noalign{\kern2pt}\nabla_vf(u,v)}\right]=\left[\matrix{4u(u^2-4)\cr\noalign{\kern2pt}2v}\right]=0$$
By examination this condition is satisfied only for $v=0$ and $u=0$, 2, and $-$2.
Therefore there are two possible stationary points.
To determine if the stationary points are minimizers, the Hessian must be examined:
$$\nabla^2f(u,v)=\left[\matrix{12u^2-16&0\cr\noalign{\kern2pt}0&2}\right]$$
At the stationary points where $u=2$ and $-$2, the Hessian is positive definite.
Therefore these points are minimizers.
At $u=0$ however, the Hessian is indefinite.
This indicates that this stationary point is a saddle point.
Therefore, $f$ has two global minimizers and one saddle point.\hfill\qed\kern3pt

{\bf c)}\hskip2mm

{\bf i.}\hskip2mm
The gradient of $f(u,v)$ is
$$\nabla f(u,v)=\left[\matrix{\nabla_ug(u)+\gamma Bv\cr\noalign{\kern2pt}\nabla_vh(v)+\gamma B^Tu}\right]$$
and the Hessian is
$$\nabla^2f(u,v)=\left[\matrix{\nabla^2_ug(u)&0\cr\noalign{\kern2pt}0&\nabla^2_vh(v)}\right]\eqno\qed\kern3pt$$

{\bf ii)}\hskip2mm
Because $g$ and $h$ are convex, $f$ is also convex.
The convexity of $g$ and $h$ implies that their Hessians have non-negative eigenvalues.
So when the diagonalized matrices $g$ and $h$ are assembled into the Hessian of $f$, it is apparent that this matrix also possesses non-negative eigenvalues.

If $\gamma=0$, the stationary points of $f$ is the intersection of the stationary points of $g$ and $h$.
So because any stationary point of $f$ is the stationary point of a convex function.
The Hessian is positive semi-definite irrespective of $\gamma$.
Therefore, any stationary point of $f$ is a global minimizer.\hfill\qed\kern3pt
\bye
