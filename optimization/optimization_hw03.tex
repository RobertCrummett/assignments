\input common/date.tex


\footline{\today\hfill Page~\folio}
\def\qed{\vrule height 6pt width 6pt depth 0pt}
\parindent 0pt
\parskip 2mm


EENG~521: Homework~3\hfill Robert~Nate~Crummett
\smallskip
\hrule


\medskip{\bf Problem~1}


The sequence $x_k=1+0.5^{2^k}$ converges to 1 quadratically.
Let the error in the sequece be $e_k=1-x_k=-0.5^{2^k}$.
That this error shrinks quadraticlly can be seen by analyzing the decrease in the magnitude of the error.
$$e_{k+1}=-0.5^{2^{k+1}}=-0.5^{2^k\cdot 2}=e_k^2$$
Therefore, at each iteration $k+1$, the error is proportional to the square of the previous error $e_k$.\hfill\qed\kern3pt


\medskip{\bf Problem~2}


The limit of the sequence $x_k={1\over k!}$ is 0, and the series approaches 0 superlinearly.
As before, I will observe the error term $e_k=0-x_k=-{1\over k!}$
The next error term can be expressed as
$$e_{k+1}=-{1\over(k+1)!}=-{1\over k!(k+1)}={e_k\over(k+1)}$$
As $k$ goes to infinity,
$$\lim_{k\to\infty}{e_{k+1}\over e_k}=\lim_{k\to\infty}{1\over k+1}=0\eqno(1)$$
This shows that the rate of convergence is faster than linear, because linear convergence would be proportional to a positive constant, not zero.
However, because the limit (1) is not proportional to $e_k$, the series does not converge quadratically.
Beause the convergenve is faster than linear, but slower than quadratic, the series converges superlinearly.\hfill\qed\kern3pt


\medskip{\bf Problem~3}


{\bf a)} The gradient and Hessian of $f_\epsilon$ are
$$\nabla f_\epsilon=\nabla f+{\epsilon\over R^2}x\qquad{\rm and}\qquad\nabla^2f_\epsilon=\nabla^2f+{\epsilon\over R^2}I$$
respectively.
Since $f(x)$ is given as convex, we know $\nabla^2f(x)\succeq0$. Therefore, $\nabla^2f_\epsilon(x)\succeq{\epsilon\over R^2}I$, since each of the eigenvalues of $\nabla^2f_\epsilon$ are all ${\epsilon\over R^2}$ larger than those of $\nabla^2f$.
But this is the definition of a strongly convex function, so $f_\epsilon$ is strongly convex.
Since $f_\epsilon$ is strongly convex, any minimizer is the unique global minimizer.
Therefore, $x_\epsilon$ is the unique global minimizer.\hfill\qed\kern3pt


{\bf b)} We are given that $f_\epsilon(x)=f(x)+{\epsilon\over2R^2}\|x\|^2_2$, so substituting this yields
$$-f(x^*)\leq{\epsilon\over2R^2}\|z\|^2_2-f(x^\epsilon)-{\epsilon\over2R^2}\|x^\epsilon\|^2_2+{\epsilon\over2}$$
where $f(z)$ cancelled on both sides.
Now the norm terms on the right hand side can be upper bounded.
Because the smallest $\|z\|^2_2-\|x^\epsilon\|^2_2$ is for $z=0$ and $x^\epsilon=R$,
$$-f(x^*)\leq{\epsilon\over2R^2}\|z\|^2_2-f(x^\epsilon)-{\epsilon\over2R^2}\|x^\epsilon\|^2_2+{\epsilon\over2}\leq-f(x^\epsilon)+{\epsilon\over2}+{\epsilon\over2}=-f(x^\epsilon)+\epsilon$$
So we have shown $f(x^\epsilon)-f(x^*)\leq\epsilon$.
This is a true statement, because $f$ was specified as a smooth function.
Therefore the initial inequality is valid.\hfill\qed\kern3pt


{\bf c)} From slide 5.29, $f_\epsilon(x_k)-f_\epsilon(x^\epsilon)\leq\left(1-{m\over L}\right)^k(f_\epsilon(x_0)-f_\epsilon(x^\epsilon))$.
Part a allows us to say $f_\epsilon$ has convexity $m={\epsilon\over R^2}$.
Thus
$$f_\epsilon(x_k)-f_\epsilon(x^\epsilon)\leq\left(1-{\epsilon\over R^2L}\right)^k(f_\epsilon(x_0)-f_\epsilon(x^\epsilon))=\left({R^2L-\epsilon\over R^2L}\right)^k(f_\epsilon(x_0)-f_\epsilon(x^\epsilon))$$
Now I am asked to solve for $k$ such that the left hand side is bounded above by $\epsilon\over2$:
$${\epsilon\over2}\leq\left({R^2L-\epsilon\over R^2L}\right)^k(f_\epsilon(x_0)-f_\epsilon(x^\epsilon))$$
The maximum disance between the optimal value and the starting value of $f_\epsilon$ is $R$. Therefore
$${\epsilon\over2}\leq\left({R^2L-\epsilon\over R^2L}\right)^kR$$
Bow we can solve for $k$. Taking the logarithm of both sides after moveing $R$ to the other side,
$$\log\left({\epsilon\over2R}\right)\leq k\log\left({R^2L-\epsilon\over R^2L}\right)$$
Now multiplying both sides by negative one and using the logarithm identity provided in the hint
$$-\log\left({\epsilon\over2R}\right)\geq -k\log\left({R^2L-\epsilon\over R^2L}\right)\geq k{\epsilon\over R^2L}$$
Finally,
$${R^2L\over\epsilon}\log\left({2R\over\epsilon}\right)\geq k$$
This is not the correct bound on $k$ --- however, this is the best I have for right now.
Encouragingly, this expression does not look very different from the provided solution, so I must have made a couple small errors somewhere in the derivation.
In broad strokes, the proof provided is probably close.\hfill\qed\kern3pt
\bye
